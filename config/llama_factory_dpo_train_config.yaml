### model
model_name_or_path: /PATH/TO/SFT/CHECKPOINT
template: qwen2_vl
image_max_pixels: 262144 # 512*512

### method
stage: dpo
pref_beta: 0.2
pref_ftx: 1.
pref_loss: sigmoid  # choices: [sigmoid (dpo), orpo, simpo]
do_train: true
finetuning_type: full
freeze_vision_tower: True
freeze_multi_modal_projector: True
deepspeed: config/deepspeed/ds_z3_config.json


### dataset
dataset_dir: third_party_packages/LLaMA-Factory/data
dataset: long_perceptual_thoughts/dpo_docci_all_extended_cots_X_O #,long_perceptual_thoughts/dpo_docci_all_extended_cots_O
cutoff_len: 2048
overwrite_cache: true
preprocessing_num_workers: 4
torch_empty_cache_steps: 1
preprocessing_batch_size: 500
tokenized_path: 

### output
output_dir: outputs/example_dpo_all_extended_cots
logging_steps: 5
save_steps: 50
save_total_limit: null
load_best_model_at_end: false
plot_loss: true
overwrite_output_dir: true

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 1
learning_rate: 0.00001
num_train_epochs: 1
max_samples: 10000000000.0
lr_scheduler_type: cosine
warmup_steps: 5
bf16: true
ddp_timeout: 180000000
eval_on_start: false
gradient_checkpointing: true

### WandB
report_to: wandb
run_name: example_dpo_all_extended_cots

flash_attn: fa2
enable_liger_kernel: true